---
title: Etica dell'IA "in futuro"
description:
hide:
- toc
---

_Testo tradotto dall'articolo online in francese su etica e apprendimento automatico [(www.lemonde.fr)](https://www.lemonde.fr/blog/binaire/2020/03/25/une-ethique-de-lia-au-futur/)._

L'intelligenza artificiale (AI) è al centro di molti dibattiti e controversie nella società. Nessun settore della vita sociale ed economica sembra essere risparmiato dall'argomento. Ciò che è interessante notare è che in tutti i dibattiti odierni intorno all'IA, l'etica è sempre chiamata in causa. Nessuno sembra ridurre la questione dell'IA a una semplice questione tecnica. Ciò è evidenziato dalla proliferazione di rapporti sull'etica dell'IA, prodotti da aziende private, attori pubblici e organizzazioni della società civile.

Tra i rapporti, le dichiarazioni possono differire un po', ma tutti sembrano sottoscrivere la stessa forma di imperativo etico. Si tratta del dovere di anticipare gli impatti delle tecnologie. L'etica - per usare le parole del filosofo Hans Jonas - deve diventare oggi "etica del futuro". Anticipare gli impatti dello sviluppo dell'IA diventa quindi un imperativo etico.

Questo imperativo non è nuovo. È in particolare al centro dell'approccio di "ricerca e innovazione responsabile" sostenuto dalla Commissione europea. Mi sembra che un concetto analogo di responsabilità sia anche al centro dei recenti rapporti francesi sull'IA. Mi concentrerò qui su tre report recenti: il report CNIL <sup>1</sup>, il report Villani <sup>2</sup>, e il report CERNA <sup>3</sup>. Cominciamo con il rapporto Villani. Questo rapporto afferma che: "la legge non può fare tutto, tra l'altro perché il tempo della legge è molto più lungo di quello del codice. È quindi essenziale che gli "architetti" della società digitale (...) si assumano la loro parte di questa missione agendo in modo responsabile. Ciò implica che siano pienamente consapevoli dei possibili effetti negativi delle loro tecnologie sulla società e che lavorino attivamente per limitarli.

Il movimento è duplice: anticipare il lato a valle dello sviluppo tecnologico, al fine di - a monte - modificare il design per evitare impatti etici negativi. Per sostenere questo approccio, il report Villani prevede di obbligare gli sviluppatori di IA a effettuare una valutazione dell'impatto della discriminazione, al fine di "costringerli a porre le domande giuste al momento giusto". Anche il report della CNIL contiene una raccomandazione simile: "Lavorare sulla progettazione di sistemi algoritmici al servizio della libertà umana". Prevedere di integrare l'etica il più presto possibile nel processo di sviluppo tecnologico. Il desiderio di implementare "l'etica nel futuro" è abbastanza lodevole, ma rimangono molte domande senza risposta che vorrei rivedere.


**Si può anticipare il futuro?**  

Prima di tutto, c'è una critica epistemologica. Come possiamo giudicare eticamente le potenzialità aperte dalla tecnologia digitale? Di fronte a un tale sviluppo, non siamo forse immersi in quella che gli economisti chiamano "incertezza radicale"? Le possibilità aperte dai "Big Data" sono un buon esempio di questa incertezza radicale. Questo termine si riferisce al fenomeno della proliferazione ininterrotta ed esponenziale dei dati. Questo sviluppo ha portato a un'evoluzione del linguaggio tecnico utilizzato per misurare il potere di memorizzazione dei dati. Dai byte si è passati ai megabyte, ai gigabyte, ecc. Come sottolinea Eric Sardin, con unità di misura come il petabyte, lo zeta-byte o lo yottabyte, è chiaro che abbiamo a che fare con unità di misura che superano puramente e semplicemente le nostre strutture umane di intelligibilità <sup>4</sup>. Inoltre, tutti i report non insistono sull'imprevedibilità di certi algoritmi di apprendimento? Consapevole di questa imprevedibilità, la CNIL difende nel suo rapporto un principio di "vigilanza" che istituisce l'obbligo di una riflessione etica continua. Ma più fondamentalmente, non dovremmo riconoscere che non tutto può essere previsto?

<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/c1d499a6-bytes.jpg" alt="Photo by Ian-S from Visualhunt.com - CC BY-NC-ND" width="500"></center>
<center>[Ian-S](https://visualhunt.co/a4/8129d567) on [Visualhunt.com](https://visualhunt.com/re6/e9ca0ca4) / [CC BY-NC-ND](http://creativecommons.org/licenses/by-nc-nd/2.0/) </center>

**Un futuro "colonizzato"?**

È anche importante rendersi conto che questo futuro che ci viene chiesto di anticipare è saturo di paure, aspettative e promesse. Il futuro non è un tempo vuoto, ma un tempo costruito da scenari, road-map e discorsi profetici. Per usare l'espressione di Didier Bigo, il futuro è "colonizzato" da numerosi attori che cercano di imporre la loro visione come matrice comune per ogni anticipazione del futuro. Inizialmente, egli ha coniato questa espressione nel contesto di una riflessione sulle tecnologie di sorveglianza, per designare le pretese e le strategie degli esperti che vedono il futuro come un "futuro prima, come un futuro già fissato, un futuro di cui conoscono gli eventi". <sup>5</sup>.

L'etica robotica corre lo stesso rischio di colonizzare il futuro, come illustrano Paul Dumouchel e Luisa Damiano nel loro libro Living with Robots. Questi ultimi pensano in particolare ad autori come Wallach e Allen, in Moral Machines, Teaching Robots Right from Wrong, che propongono un programma per insegnare ai robot la differenza tra giusto e sbagliato, per renderli "agenti morali artificiali". Ma "programmare" un agente moralmente autonomo non è una contraddizione in termini? Non intendo entrare in un dibattito metafisico su questo argomento. Piuttosto, vorrei far notare che un tale programma ha letteralmente 'colonizzato' l'orizzonte dei dibattiti sull'etica robotica. Seguiamo Dumouchel e Damiano per cogliere questo punto. Essi notano che, nell'opinione di alcuni dei protagonisti dell'etica robotica, "siamo ancora lontani dal poter creare agenti artificiali autonomi che possano essere veri agenti morali. Non sappiamo nemmeno se saremo mai in grado di farlo", dicono. Non sappiamo se tali macchine sono possibili". <sup>6</sup>. Una delle risposte avanzate dai sostenitori dell'etica robotica è quindi che "non dovremmo aspettare di elaborare tali regole fino a quando non saremo colti alla sprovvista dall'improvvisa irruzione di agenti artificiali autonomi". È importante già prepararsi per un futuro inevitabile. I filosofi devono già partecipare allo sviluppo dei robot che popoleranno la nostra vita quotidiana in futuro, sviluppando strategie per inscrivere regole morali nei robot che vincolino il loro comportamento" <sup>7</sup>.

Questo tema dell'inevitabile potenziamento delle macchine è potente e abbastanza problematico. Di tutte le possibilità, l'attenzione si concentra su questo scenario, che è posto come "inevitabile". Una tale focalizzazione dell'attenzione pone diversi problemi. Da un lato, solleva questioni epistemologiche: perché è possibile sostenere questa inevitabilità? D'altra parte, per Dumouchel e Damiano, questa convinzione, anche se non razionale, ha alcuni effetti reali: distoglie l'attenzione dalle questioni di potere che sono già in gioco, e cioè che l'empowerment dei robot, il fatto di delegare loro le decisioni e lasciare che scelgano da soli, può significare la perdita di potere decisionale per alcuni, ma intensifica anche la concentrazione del processo decisionale nelle mani di pochi (programmatori, proprietari di robot, ecc.).

<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/7b8b81cc-forbiden-planet.jpg" alt="Photo on data representation" width="500"></center>
<center>Image form the film  Forbiden Planet (USA, 1956) – MGM productions</center>

**Chi anticipa?**

Una domanda correlata è chi farà questa anticipazione degli impatti etici? Chi saranno gli autori degli impatti discriminatori? Solo i ricercatori?  Su questo punto, tutti i report chiariscono che questo imperativo etico di anticipazione non riguarda solo i ricercatori. Il report del CERNA specifica che "i ricercatori devono deliberare fin dall'inizio del loro progetto con le persone o i gruppi identificati come suscettibili di essere influenzati". Per il report Villani, "dobbiamo creare un vero forum di dibattito, plurale e aperto alla società, per determinare democraticamente quale IA vogliamo per la nostra società". Quanto alla CNIL, afferma nel suo report che "I sistemi algoritmici e di intelligenza artificiale sono oggetti socio-tecnici complessi, modellati e manipolati da lunghe e complesse catene di attori. È quindi lungo tutta la catena algoritmica (dal progettista all'utente finale, passando per coloro che formano i sistemi e coloro che li impiegano) che bisogna agire, utilizzando una combinazione di approcci tecnici e organizzativi. Gli algoritmi sono ovunque: sono quindi affari di tutti".

I report citati prestano attenzione alla questione del CHI anticipa. Tuttavia, la richiesta di deliberare con "individui o gruppi identificati come potenzialmente influenzabili" (CERNA) non è sufficiente. La determinazione dell'elenco delle persone interessate deve essere oggetto di riflessione e di ricerca, non un prerequisito di questo processo di anticipazione. In effetti, l'anticipazione di questi impatti può farci conoscere nuovi interlocutori da includere nella riflessione. Inoltre, che forma dare a un'etica dell'IA che sia "affare di tutti"? Come può essere istituita?

**Una considerazione temporale che oscura lo spazio**

Infine, un'ultima questione rimasta senza risposta è il privilegio dato al tempo rispetto allo spazio nella riflessione etica. Lo sviluppo della tecnologia digitale, e in particolare dell'IA, ha contribuito all'idea che la virtualizzazione degli scambi avrebbe ridotto le distanze e l'importanza dei luoghi. Tale convinzione è, ad esempio, alla base dello sviluppo della telemedicina: un malato cronico può essere monitorato indifferentemente a casa o in un'istituzione, uno specialista può essere chiamato per una "tele-perizia" indipendentemente dalla distanza che lo separa dal suo collega, ecc. Tuttavia, come affermano i sociologi Alexandre Mathieu-Fritz e Gérald Gaglio, "la telemedicina non porta all'abolizione delle frontiere e degli spazi, contrariamente alla visione di senso comune spesso implicitamente trasmessa dalle politiche pubbliche"<sup>8</sup>. Per essere efficace, la telemedicina richiede una certa pianificazione spaziale. In uno studio etnografico condotto con pazienti telemonitorati, Nelly Oudshoorn ha mostrato la misura in cui i luoghi, lo spazio domestico e lo spazio pubblico influenzano e modellano il modo in cui le tecnologie vengono implementate, così come, al contrario, queste tecnologie trasformano letteralmente questi spazi. La casa diventa così un luogo ibrido, uno spazio vitale medicalizzato. Questa dimensione spaziale non sembra essere presa in considerazione.

Tuttavia, un autore come Pierre Rosanvallon ci ha ricordato dieci anni fa nel suo libro La légitimité démocratique che la legittimità dell'azione pubblica oggi dipende sempre più da quello che lui chiama un "principio di prossimità", un'attenzione ai contesti locali. Ciò è dimostrato dalla promozione di un approccio di "sperimentazione" nel campo dell'IA in ambito sanitario: "per beneficiare dei progressi dell'IA in medicina, è importante facilitare le sperimentazioni della tecnologia dell'IA in ambito sanitario in tempo reale e il più vicino possibile agli utenti, creando le condizioni normative e organizzative necessarie" (report Villani). In modo più induttivo e partendo dal campo, si tratterebbe di favorire una nuova costruzione dell'azione pubblica: "sperimentare l'azione pubblica" come ci invitano a fare Clément Bertholet e Laura Létourneau <sup>9</sup>.

Mentre questo approccio sperimentale ci invita a prendere in considerazione le realtà contestuali locali, l'obiettivo è sempre la 'scalabilità' (il fatto che possa essere utilizzato a diverse scale). Prendo questo termine dall'antropologa Anna Lowenhaupt Tsing, che lo definisce come: "la capacità dei progetti di espandersi senza cambiare il quadro delle loro ipotesi" <sup>10</sup>. È vero che sperimentiamo localmente, ma mi sembra che lo facciamo per individuare ciò che può essere generalizzato e reso operativo in modo indifferenziato a diverse scale. Un sistema di monitoraggio a distanza - per fare questo esempio - può essere applicato in qualsiasi contesto? Tutti gli spazi di vita possono diventare luoghi di cura?

L'etica dell'IA tiene conto del fatto che gli impatti dell'IA saranno diversi a seconda dei luoghi e degli spazi? Dove dovrebbero avvenire le valutazioni etiche dell'impatto? Dovrebbero essere centralizzate o localizzate nei luoghi in cui vengono progettati gli oggetti tecnologici? Più fondamentalmente, è possibile determinare questi impatti senza muoversi? Su questo punto, la mappatura del panorama globale dell'etica dell'IA realizzata da A. Jobin et al <sup>11</sup> è abbastanza istruttiva. Essi hanno condotto un'analisi comparativa di 84 report sull'etica dell'IA. Si tratta di documenti prodotti da agenzie governative, aziende private, organizzazioni non-profit e società scientifiche. La loro analisi evidenzia il fatto che la maggior parte dei report sono prodotti negli Stati Uniti (20 report), nell'Unione europea (19), seguita da Regno Unito (14) e Giappone (4). I paesi africani e latinoamericani non sono rappresentati indipendentemente dalle organizzazioni internazionali o sovranazionali. Questa distribuzione geografica non indica forse che questa dimensione spaziale viene trascurata?

**Alain Loute** (Centre d’éthique médicale, labo ETHICS EA 7446, Université Catholique de Lille - Centre for Medical Ethics /Catholic University of Lille)

<sup>1</sup>(https://www.lemonde.fr/#_ftnref1) [Comment permettre à l’homme de garder la main](https://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_garder_la_main_web.pdf) ? Les enjeux éthiques des algorithmes et de l’intelligence artificielle, CNIL, 2017 (How to enable man to keep his hand in, in.The ethical challenges of algorithms and artificial intelligence).

<sup>2</sup>(https://www.lemonde.fr/#_ftnref2) [Donner un sens à l’intelligence artificielle, Pour une stratégie nationale et européenne](https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf), Rapport Villani, 8 mars 2018 (Giving meaning to artificial intelligence, For a national and European strategy).

<sup>3</sup>(https://www.lemonde.fr/#_ftnref3) [Ethique de la recherche en apprentissage machine](http://cerna-ethics-allistene.org/digitalAssets/53/53991_cerna___thique_apprentissage.pdf), Avis de la Commission de réflexion sur l’Ethique de la Recherche en sciences et technologies du Numérique d’Allistene (CERNA), juin 2017 (Ethics of machine learning research, in.Opinion of the Allistene Commission on Research Ethics in Digital Science and Technology).

<sup>4</sup> E. Sardin, La vie algorithmique, Critique de la raison numérique, Ed. L’échappée, Paris 2015, pp. 21-22 (Algorithmic Life, Critique of Digital Reason).

<sup>5</sup> D. Bigo, « Sécurité maximale et prévention ? La matrice du futur antérieur et ses grilles », in B. Cassin (éd.), Derrière les grilles, Sortons du tout-évaluation, Paris, Fayard, 2014, p. 111-138, p. 126 (Maximum security and prevention? The future tense matrix and its grids).

<sup>6</sup> P. Dumouchel et L. Damiano, Vivre avec des robots, Essai sur l’empathie artificielle, Paris, Seuil, 2016, p. 191 (Living with robots, An essay on artificial empathy).

<sup>7</sup> ibid., p. 191-192.

<sup>8</sup> A. Mathieu-Fritz et G. Gaglio, « À la recherche des configurations sociotechniques de la télémédecine, Revue de littérature des travaux de sciences sociales », in Réseaux, 207, 2018/1, pp. 27-63 (In search of the socio-technical configurations of telemedicine).

<sup>9</sup> C. Bertholet et L. Létourneau, Ubérisons l’Etat avant que les autres ne s’en chargent, Paris, Armand Collin, 2017, p.182 (Let's uberise the state before others do it).

<sup>10</sup> A. Lowenhaupt Tsing, Le champignon de la fin du monde, Sur la possibilité de vivre dans les ruines du capitalisme, Paris, La Découverte, 2017, p. 78(The mushroom at the end of the world, On the possibility of living in the ruins of capitalism).

<sup>11</sup> A. Jobin, M. Ienca, & E. Vayena, « The global landscape of AI ethics guidelines », in Nat Mach Intell, 1, 2019, pp. 389–399.
