---
title: Etica dell'IA "in futuro"
description:
hide:
- toc
---

_Testo tradotto dall'articolo online in francese su etica e apprendimento automatico [(www.lemonde.fr)](https://www.lemonde.fr/blog/binaire/2020/03/25/une-ethique-de-lia-au-futur/)._

L'intelligenza artificiale (AI) è al centro di molti dibattiti e controversie nella società. Nessun settore della vita sociale ed economica sembra essere risparmiato dall'argomento. Ciò che è interessante notare è che in tutti i dibattiti odierni intorno all'IA, l'etica è sempre chiamata in causa. Nessuno sembra ridurre la questione dell'IA a una semplice questione tecnica. Ciò è evidenziato dalla proliferazione di rapporti sull'etica dell'IA, prodotti da aziende private, attori pubblici e organizzazioni della società civile.

Tra i rapporti, le dichiarazioni possono differire un po', ma tutti sembrano sottoscrivere la stessa forma di imperativo etico. Si tratta del dovere di anticipare gli impatti delle tecnologie. L'etica - per usare le parole del filosofo Hans Jonas - deve diventare oggi "etica del futuro". Anticipare gli impatti dello sviluppo dell'IA diventa quindi un imperativo etico.

Questo imperativo non è nuovo. È in particolare al centro dell'approccio di "ricerca e innovazione responsabile" sostenuto dalla Commissione europea. Mi sembra che un concetto analogo di responsabilità sia anche al centro dei recenti rapporti francesi sull'IA. Mi concentrerò qui su tre report recenti: il report CNIL <sup>1</sup>, il report Villani <sup>2</sup>, e il report CERNA <sup>3</sup>. Cominciamo con il rapporto Villani. Questo rapporto afferma che: "la legge non può fare tutto, tra l'altro perché il tempo della legge è molto più lungo di quello del codice. È quindi essenziale che gli "architetti" della società digitale (...) si assumano la loro parte di questa missione agendo in modo responsabile. Ciò implica che siano pienamente consapevoli dei possibili effetti negativi delle loro tecnologie sulla società e che lavorino attivamente per limitarli.

Il movimento è duplice: anticipare il lato a valle dello sviluppo tecnologico, al fine di - a monte - modificare il design per evitare impatti etici negativi. Per sostenere questo approccio, il report Villani prevede di obbligare gli sviluppatori di IA a effettuare una valutazione dell'impatto della discriminazione, al fine di "costringerli a porre le domande giuste al momento giusto". Anche il report della CNIL contiene una raccomandazione simile: "Lavorare sulla progettazione di sistemi algoritmici al servizio della libertà umana". Prevedere di integrare l'etica il più presto possibile nel processo di sviluppo tecnologico. Il desiderio di implementare "l'etica nel futuro" è abbastanza lodevole, ma rimangono molte domande senza risposta che vorrei rivedere.


**Si può anticipare il futuro?**  

Prima di tutto, c'è una critica epistemologica. Come possiamo giudicare eticamente le potenzialità aperte dalla tecnologia digitale? Di fronte a un tale sviluppo, non siamo forse immersi in quella che gli economisti chiamano "incertezza radicale"? Le possibilità aperte dai "Big Data" sono un buon esempio di questa incertezza radicale. Questo termine si riferisce al fenomeno della proliferazione ininterrotta ed esponenziale dei dati. Questo sviluppo ha portato a un'evoluzione del linguaggio tecnico utilizzato per misurare il potere di memorizzazione dei dati. Dai byte si è passati ai megabyte, ai gigabyte, ecc. Come sottolinea Eric Sardin, con unità di misura come il petabyte, lo zeta-byte o lo yottabyte, è chiaro che abbiamo a che fare con unità di misura che superano puramente e semplicemente le nostre strutture umane di intelligibilità <sup>4</sup>. Inoltre, tutti i report non insistono sull'imprevedibilità di certi algoritmi di apprendimento? Consapevole di questa imprevedibilità, la CNIL difende nel suo rapporto un principio di "vigilanza" che istituisce l'obbligo di una riflessione etica continua. Ma più fondamentalmente, non dovremmo riconoscere che non tutto può essere previsto?

<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/c1d499a6-bytes.jpg" alt="Photo by Ian-S from Visualhunt.com - CC BY-NC-ND" width="500"></center>
<center>[Ian-S](https://visualhunt.co/a4/8129d567) on [Visualhunt.com](https://visualhunt.com/re6/e9ca0ca4) / [CC BY-NC-ND](http://creativecommons.org/licenses/by-nc-nd/2.0/) </center>

**Un futuro "colonizzato"?**

È anche importante rendersi conto che questo futuro che ci viene chiesto di anticipare è saturo di paure, aspettative e promesse. Il futuro non è un tempo vuoto, ma un tempo costruito da scenari, road-map e discorsi profetici. Per usare l'espressione di Didier Bigo, il futuro è "colonizzato" da numerosi attori che cercano di imporre la loro visione come matrice comune per ogni anticipazione del futuro. Inizialmente, egli ha coniato questa espressione nel contesto di una riflessione sulle tecnologie di sorveglianza, per designare le pretese e le strategie degli esperti che vedono il futuro come un "futuro prima, come un futuro già fissato, un futuro di cui conoscono gli eventi". <sup>5</sup>.

The robotic ethic runs the same risk of colonising the future, as Paul Dumouchel and Luisa Damiano illustrate in their book Living with Robots. The latter are thinking in particular of authors such as Wallach and Allen, in Moral Machines, Teaching Robots Right from Wrong, who propose a programme to teach robots the difference between right and wrong, to make them "artificial moral agents". But isn't 'programming' a morally autonomous agent a contradiction in terms? I do not intend to enter into a metaphysical debate on this subject. Rather, I would like to point out that such a programme has literally 'colonised' the horizon of expectation of debates on robotic ethics. Let us follow Dumouchel and Damiano to grasp this point. They note that, in the opinion of some of the protagonists of robotic ethics, "we are still far from being able to create autonomous artificial agents that could be true moral agents. We don't even know if we will ever be able to do that," they say. We don't know if such machines are possible" <sup>6</sup>. One of the answers put forward by the proponents of robotic ethics is then that "we should not wait to elaborate such rules until we are caught unawares by the sudden irruption of autonomous artificial agents. It is important already to prepare for an inevitable future. Philosophers must already participate in the development of the robots that will populate our daily lives in the future by developing strategies to inscribe moral rules in robots that constrain their behaviour" <sup>7</sup>.

This theme of the inevitable empowerment of machines is powerful and quite problematic. Of all the possibilities, attention is focused on this scenario, which is posited as 'inevitable'. Such a focus of attention poses several problems. On the one hand, it raises epistemological questions: why is it possible to support this inevitability? On the other hand, for Dumouchel and Damiano, this belief, although not rational, has certain real effects: it diverts attention from power issues that are already at stake, namely that the empowerment of robots, the fact of delegating decisions to them and letting them choose for themselves, may mean the loss of decision-making power for a few, but it also intensifies the concentration of decision-making in the hands of a few (programmers, robot owners, etc.).

<center><img src="https://storage.googleapis.com/prd-blogs/2020/03/7b8b81cc-forbiden-planet.jpg" alt="Photo on data representation" width="500"></center>
<center>Image form the film  Forbiden Planet (USA, 1956) – MGM productions</center>

**Who anticipates?**

A related question is who will do this anticipation of ethical impacts? Who will be the perpetrators of the discriminatory impacts? Is it only researchers?  On this point, all the reports make it clear that this ethical imperative to anticipate does not only concern researchers. The CERNA report specifies that "researchers must deliberate from the outset of their project with the persons or groups identified as being likely to be influenced". For the Villani report, "We must create a real forum for debate, plural and open to society, in order to determine democratically what AI we want for our society". As for the CNIL, it states in its report that "Algorithmic and artificial intelligence systems are complex socio-technical objects, shaped and manipulated by long and complex chains of actors. It is therefore all along the algorithmic chain (from the designer to the end user, through those who train the systems and those who deploy them) that action must be taken, using a combination of technical and organisational approaches. Algorithms are everywhere: they are therefore everyone's business".

The reports cited do pay attention to the question of WHO anticipates. However, the injunction to deliberate with "individuals or groups identified as potentially being influenced" (CERNA) is not enough. Determining the list of people concerned must be an object of reflection and research, not a prerequisite for this anticipation process. Indeed, anticipating these impacts can make us aware of new stakeholders to be included in the reflection. Moreover, what form should be given to an ethics of AI that is "everybody's business"? How can it be instituted?

**A consideration of time that obscures space**

Finally, a last question left unanswered is the privilege given to time over space in ethical reflection. The development of digital technology, and in particular AI, has contributed to the idea that the virtualisation of exchanges would reduce distances and the importance of places. Such a belief is, for example, at the heart of the development of telemedicine: a chronically ill person can be monitored indifferently at home or in an institution, a specialist can be called upon by tele-expertise regardless of the distance separating him from his colleague, etc. However, as the sociologists Alexandre Mathieu-Fritz and Gérald Gaglio state, "telemedicine does not lead to the abolition of borders and spaces, contrary to the common sense view often implicitly conveyed by public policies"<sup>8</sup>. In order to be effective, telemedicine requires a certain amount of spatial planning. In an ethnographic study carried out with telemonitored patients, Nelly Oudshoorn has shown the extent to which places, domestic space and public space influence and shape the way in which technologies are implemented, just as, conversely, these technologies literally transform these spaces. The home thus becomes a hybrid place, a medicalised living space. This spatial dimension does not really seem to be taken into account.

However, an author such as Pierre Rosanvallon reminded us ten years ago in his book La légitimité démocratique that the legitimacy of public action today depends more and more on what he calls a 'principle of proximity', an attention to local contexts. This is demonstrated by the promotion of an "experimentation" approach in the field of AI in health: "in order to benefit from the advances of AI in medicine, it is important to facilitate experimentations of AI technology in health in real time and as close as possible to the users, by creating the necessary regulatory and organisational conditions" (Villani Report). In a more inductive way and starting from the field, it would be a question of favouring a new construction of public action: "experimenting with public action" as Clément Bertholet and Laura Létourneau invite us to do <sup>9</sup>.

While this experimental approach invites us to take into account local contextual realities, the objective is always 'scalability' (the fact that it can be used at different scales). I take this term from the anthropologist Anna Lowenhaupt Tsing, who defines it as: "the capacity of projects to expand without changing the framework of their hypothesis" <sup>10</sup>. It is true that we experiment locally, but it seems to me that we do so in order to identify what can be generalised and made operational in an undifferentiated way at different scales. Can a remote monitoring system - to take this example - be applied in any context? Can all living spaces become places of care?

Does the ethics of AI take into account the fact that the impacts of AI will differ according to places and spaces? Where should ethical impact assessments take place? Should they be centralised or localised in the places where technical objects are designed? More fundamentally, is it possible to determine these impacts without moving? On this point, the mapping of the global landscape of AI ethics carried out by A. Jobin et al <sup>11</sup> is quite instructive. They conducted a comparative analysis of 84 reports on AI ethics. These are documents produced by government agencies, private firms, non-profit organisations and learned societies. Their analysis highlights the fact that the majority of reports are produced in the USA (20 reports), the European Union (19), followed by the UK (14) and Japan (4). African and Latin American countries are not represented independently of international or supra-national organisations. Does this geographical distribution not indicate that this spatial dimension is being overlooked?

**Alain Loute** (Centre d’éthique médicale, labo ETHICS EA 7446, Université Catholique de Lille - Centre for Medical Ethics /Catholic University of Lille)

<sup>1</sup>(https://www.lemonde.fr/#_ftnref1) [Comment permettre à l’homme de garder la main](https://www.cnil.fr/sites/default/files/atoms/files/cnil_rapport_garder_la_main_web.pdf) ? Les enjeux éthiques des algorithmes et de l’intelligence artificielle, CNIL, 2017 (How to enable man to keep his hand in, in.The ethical challenges of algorithms and artificial intelligence).

<sup>2</sup>(https://www.lemonde.fr/#_ftnref2) [Donner un sens à l’intelligence artificielle, Pour une stratégie nationale et européenne](https://www.aiforhumanity.fr/pdfs/9782111457089_Rapport_Villani_accessible.pdf), Rapport Villani, 8 mars 2018 (Giving meaning to artificial intelligence, For a national and European strategy).

<sup>3</sup>(https://www.lemonde.fr/#_ftnref3) [Ethique de la recherche en apprentissage machine](http://cerna-ethics-allistene.org/digitalAssets/53/53991_cerna___thique_apprentissage.pdf), Avis de la Commission de réflexion sur l’Ethique de la Recherche en sciences et technologies du Numérique d’Allistene (CERNA), juin 2017 (Ethics of machine learning research, in.Opinion of the Allistene Commission on Research Ethics in Digital Science and Technology).

<sup>4</sup> E. Sardin, La vie algorithmique, Critique de la raison numérique, Ed. L’échappée, Paris 2015, pp. 21-22 (Algorithmic Life, Critique of Digital Reason).

<sup>5</sup> D. Bigo, « Sécurité maximale et prévention ? La matrice du futur antérieur et ses grilles », in B. Cassin (éd.), Derrière les grilles, Sortons du tout-évaluation, Paris, Fayard, 2014, p. 111-138, p. 126 (Maximum security and prevention? The future tense matrix and its grids).

<sup>6</sup> P. Dumouchel et L. Damiano, Vivre avec des robots, Essai sur l’empathie artificielle, Paris, Seuil, 2016, p. 191 (Living with robots, An essay on artificial empathy).

<sup>7</sup> ibid., p. 191-192.

<sup>8</sup> A. Mathieu-Fritz et G. Gaglio, « À la recherche des configurations sociotechniques de la télémédecine, Revue de littérature des travaux de sciences sociales », in Réseaux, 207, 2018/1, pp. 27-63 (In search of the socio-technical configurations of telemedicine).

<sup>9</sup> C. Bertholet et L. Létourneau, Ubérisons l’Etat avant que les autres ne s’en chargent, Paris, Armand Collin, 2017, p.182 (Let's uberise the state before others do it).

<sup>10</sup> A. Lowenhaupt Tsing, Le champignon de la fin du monde, Sur la possibilité de vivre dans les ruines du capitalisme, Paris, La Découverte, 2017, p. 78(The mushroom at the end of the world, On the possibility of living in the ruins of capitalism).

<sup>11</sup> A. Jobin, M. Ienca, & E. Vayena, « The global landscape of AI ethics guidelines », in Nat Mach Intell, 1, 2019, pp. 389–399.
