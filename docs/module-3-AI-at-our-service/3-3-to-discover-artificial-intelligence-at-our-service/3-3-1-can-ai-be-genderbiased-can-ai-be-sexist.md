---
title: L'IA può essere di genere?
description:
hide:
- toc
---

_Testo tradotto dal Mooc francese IAI._

<center><img src="../Images/ondira-levrres-intelligence-artificielle-femmes.jpg" alt="Photo by Fernando Arcos from Pexels" width="500"></center>
<center>[Photo: CBC/Radio-Canada -iStock - Editing: Amarilys Proulx]()</center>

Un'intelligenza artificiale non pensa. Quindi può essere razzista, sessista o omofoba? È possibile che riproduca meccanismi discriminatori, se il meccanismo è stato addestrato da dati in cui si annidano dei bias. Gli esseri umani hanno bias cognitivi nel loro ragionamento.

I **bias cognirivi** sono l'insieme dei fattori psicologici, morali, sociali, culturali... che influenzano, senza che ce ne rendiamo conto, i nostri meccanismi di pensiero. Questi bias cognitivi hanno un impatto sul pensiero logico, poiché influenzano il nostro modo di pensare e ci portano a prendere decisioni guidate da fattori non razionali. Possono quindi essere la causa di decisioni discriminatorie.

Ma siamo noi a fornire i dati agli algoritmi. Se ci sono bias nei dati forniti, questo si rifletterà nei risultati. Per esempio, l'azienda Amazon ha esigenze di reclutamento standard, e per facilitarne la gestione, ha deciso di utilizzare un'intelligenza artificiale in grado di ordinare i CV. Affinché questa IA imparasse a ordinare i profili, Amazon ha fornito tutti i dati di reclutamento che aveva registrato tra il 2004 e il 2014. Tuttavia, dopo un anno, Amazon si rese conto che l'IA rifiutava sistematicamente i profili femminili per le posizioni tecniche e IT. L'intelligenza artificiale aveva semplicemente determinato, analizzando i dati che le erano stati forniti, che il reclutamento per queste posizioni negli ultimi dieci anni era in gran parte maschile<sup>1</sup>. Amazon ha fermato questa intelligenza artificiale perché i risultati ottenuti presentavano bias di genere. Ma non si tratta solo di bias nei dati, ci sono anche bias nel modo in cui gli algoritmi stessi sono implementati.

Le tecnologie non sono quindi neutre; riflettono indirettamente i modi in cui operano i loro progettisti e le nostre società. Anche fattori come la lingua possono avere un'influenza su di esse. In Francia, la composizione della lingua favorisce l'apprendimento di genere più che in altri paesi. Poiché i nomi sono prevalentemente maschili, un algoritmo conosce più nomi maschili che femminili. Così, i siti di traduzione, come Google translation, tendono a dare una traduzione maschile di nomi che sono di genere neutro in un'altra lingua<sup>2</sup>.

L'intelligenza artificiale può rafforzare le disuguaglianze così come combatterle. Di fronte a queste sfide etiche e tecniche, sono previste diverse modalità di azione. Per esempio, garantire una maggiore diversità di genere nelle scienze informatiche, in modo che gli uomini non siano più sovrarappresentati. Oppure superare i bias cognitivi insegnando all'algoritmo a riconoscerli. Questa seconda opzione richiede che i progettisti considerino a monte i bias cognitivi, anche se non sempre ne sono consapevoli. La soluzione migliore è garantire la diversità nei dati di apprendimento, poiché i dati possono essere più facili da cambiare rispetto alle mentalità.<sup>3</sup>

* * *

<sup>1</sup> _[Amazon a dû se débarrasser d’une intelligence artificielle sexiste”,](http://www.slate.fr/story/168413/amazon-abandonne-intelligence-artificielle-sexiste) Slatefr,10 octobre 2018. ; [Amazon met fin à une intelligence artificielle sexiste”](https://www.franceinter.fr/emissions/c-est-deja-demain/c-est-deja-demain-12-octobre-2018) par Hélène Chevallier, France Inter, emission c'est déjà demain, vendredi 12 octobre 2018 ; [Amazon a du débrancher un logiciel de recrutement qui s’est révélé sexiste”](https://www.europe1.fr/emissions/axel-de-tarle-vous-parle-economie/amazon-a-du-debrancher-une-logiciel-de-recrutement-qui-sest-revele-sexiste-37768493), La matinale d'Europe 1, le 6h - 9h, l'édito économique d'Axel de Tarlé, le 12 octobre 2018._  
<sup>2</sup> _[Quand l’Intelligence Artificielle rencontre les Sciences du Langage”](https://chut.media/portraits/intelligence-artificielle-sciences-du-langage/) par Aurore Bisicchia, Chut!, janvier 2005._  
<sup>3</sup> _[“L’IA est–elle sexiste, elle aussi ?”](https://www.lemonde.fr/blog/binaire/2018/03/08/lia-est-elle-sexiste-elle-aussi/) par Anne-Marie Kermarrec, Blog Binaire, Le Monde, 08 mars 2018._  
_["Les biais sexistes de l'IA peuvent être corrigés, selon les chercheuses Aude Bernheim et Flora Vincent"](https://www.usinenouvelle.com/editorial/les-biais-sexistes-de-l-ia-peuvent-etre-corriges-selon-les-chercheuses-aude-bernheim-et-flora-vincent.N815345), L'Usine Nouvelle, propos recueillis par Marion Garreau, 08/03/2019._  
_["Le secteur de l’intelligence artificielle est aussi masculin qu’un bar des sports le soir d’un match de Ligue 1”](https://www.lopinion.fr/edition/politique/secteur-l-intelligence-artificielle-est-aussi-masculin-qu-bar-sports-180114), Interview d'Aude Bernheim et Flora Vincent par Irène Inchauspé, L’opinion, 08 Mars 2019._
